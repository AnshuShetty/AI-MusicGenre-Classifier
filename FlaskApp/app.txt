import os
import numpy as np
import librosa
import tensorflow as tf
from io import BytesIO
from flask import Flask, request, jsonify
from flask_cors import CORS  # For handling Cross-Origin Resource Sharing (CORS)
from scipy.ndimage import zoom

# Flask app setup
app = Flask(__name__)
# CORS configuration
CORS(app, resources={r"/predict": {"origins": "http://localhost:3000"}})  # Allow specific origin

# Load the trained model
MODEL_PATH = './Trained_model.h5'  # Path to your .h5 file
model = tf.keras.models.load_model(MODEL_PATH)

# Genre labels - map model output to actual genre names
genre_labels = [
    "Pop",      # 0
    "Rock",     # 1
    "Jazz",     # 3
    "Hip Hop",  # 2
    "Electronic", # 5
    "Classical",# 4
]

# Function for resizing the Mel spectrogram
def resize(image, target_shape):
    return zoom(image, (target_shape[0] / image.shape[0], target_shape[1] / image.shape[1], 1))

# Load and preprocess audio data
def load_and_preprocess_data(file_stream, target_shape=(150, 150)):
    data = []
    audio_data, sample_rate = librosa.load(file_stream, sr=None)
    
    # Define the duration of each chunk and overlap
    chunk_duration = 4  # seconds
    overlap_duration = 2  # seconds
    
    # Convert durations to samples
    chunk_samples = chunk_duration * sample_rate
    overlap_samples = overlap_duration * sample_rate
    
    # Calculate the number of chunks
    num_chunks = int(np.ceil((len(audio_data) - chunk_samples) / (chunk_samples - overlap_samples))) + 1
    
    # Iterate over each chunk
    for i in range(num_chunks):
        # Calculate start and end indices of the chunk
        start = i * (chunk_samples - overlap_samples)
        end = start + chunk_samples
        
        # Extract the chunk of audio
        chunk = audio_data[start:end]
        
        # Compute the Mel spectrogram for the chunk
        mel_spectrogram = librosa.feature.melspectrogram(y=chunk, sr=sample_rate)
        
        # Resize the Mel spectrogram
        mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)
        data.append(mel_spectrogram)
    
    return np.array(data)

# Model Prediction
def model_prediction(X_test):
    y_pred = model.predict(X_test)
    predicted_categories = np.argmax(y_pred, axis=1)
    unique_elements, counts = np.unique(predicted_categories, return_counts=True)
    max_count = np.max(counts)
    max_elements = unique_elements[counts == max_count]
    return max_elements[0]  # Return the most frequent genre

# Flask route for prediction
@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({"error": "No file uploaded"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No file selected"}), 400

    # Read the file directly into memory
    file_stream = BytesIO(file.read())
    
    try:
        # Preprocess the audio and make predictions
        X_test = load_and_preprocess_data(file_stream)
        predicted_genre_index = model_prediction(X_test)
        
        # Convert numpy int64 to a Python int and map to genre name
        predicted_genre_index = int(predicted_genre_index)  # Ensure it's a native Python int
        predicted_genre = genre_labels[predicted_genre_index]  # Map index to genre name
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    return jsonify({"genre": predicted_genre})

if __name__ == '__main__':
    app.run(debug=True, port=7000)
